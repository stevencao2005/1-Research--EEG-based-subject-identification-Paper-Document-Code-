{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevencao2005/DeepLearningModels_On_EEG_Data/blob/main/Deep_Learning_Networks_on_EEG_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrIkj9p3mpeQ"
      },
      "source": [
        "## Importing the modules and python files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn8t5ojbZzer"
      },
      "outputs": [],
      "source": [
        "#IMPORT ALL NEEDED MODULES\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import sklearn\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7xLrMsfbNPi",
        "outputId": "f113cac6-3898-450f-9d02-8e38efede042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#MOUNT GOOGLE DRIVE TO IMPORT FILES IN OUR GOOGLE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2RyusOFho0j"
      },
      "outputs": [],
      "source": [
        "#ALLOWING THE IMPORT OF PYTHON FILES\n",
        "!cp /content/gdrive/MyDrive/DeepLearningModels_On_EEG_Data/utils.py .\n",
        "sys.argv.extend('/content/gdrive/MyDrive/DeepLearningModels_On_EEG_Data/')\n",
        "sys.argv.extend('/content/gdrive/MyDrive/DeepLearningModels_On_EEG_Data/classifiers')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU00bjpAxM0_"
      },
      "outputs": [],
      "source": [
        "!cp /content/gdrive/MyDrive/DeepLearningModels_On_EEG_Data/classifiers/inception.py .\n",
        "!cp /content/gdrive/MyDrive/DeepLearningModels_On_EEG_Data/classifiers/resnet.py .\n",
        "!cp /content/gdrive/MyDrive/DeepLearningModels_On_EEG_Data/classifiers/EEGModels.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRqrh4zklA67"
      },
      "outputs": [],
      "source": [
        "#IMPORTING PYTHON FILES\n",
        "import utils\n",
        "from utils import generate_results_csv\n",
        "from utils import create_directory\n",
        "from utils import read_dataset\n",
        "from utils import transform_mts_to_ucr_format\n",
        "from utils import visualize_filter\n",
        "from utils import viz_for_survey_paper\n",
        "from utils import calculate_metrics\n",
        "from utils import case_by_case_analysis\n",
        "from utils import viz_cam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYWI3Q1jmkV_"
      },
      "source": [
        "## Functions used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0f1tTH9aE8w"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoder(y_train, y_test):\n",
        "    enc     = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
        "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
        "    y_test  = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
        "    return y_train, y_test\n",
        "def standardizing_the_dataset(X_train):\n",
        "    sc = StandardScaler()\n",
        "    sc.fit(X_train)\n",
        "    X_train = sc.transform(X_train)\n",
        "    return X_train\n",
        "#FOR FITTING THE MODEL ONTO THE DATASET\n",
        "def fit_classifier():\n",
        "    #GETTING THE TRAIN/TEST DATASETS\n",
        "    x_train = datasets_dict[dataset_name][0]\n",
        "    y_train = datasets_dict[dataset_name][1]\n",
        "    x_test  = datasets_dict[dataset_name][2]\n",
        "    y_test  = datasets_dict[dataset_name][3]\n",
        "\n",
        "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
        "\n",
        "    #TRANSFROM THE LABELS FROM INTEGERS TO ONE HOT VECTORS\n",
        "    y_train, y_test = one_hot_encoder(y_train, y_test)\n",
        "\n",
        "    #standardizing the train/test datasets for each channel\n",
        "    x_train_preprocessed = np.zeros((x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
        "    x_test_preprocessed  = np.zeros((x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
        "    channels             = [i for i in range(x_test.shape[1])]\n",
        "    for channel in channels:\n",
        "        x_train_channelData       = x_train[:,channel,:]\n",
        "        x_test_channelData        = x_test[:,channel,:]\n",
        "        x_train_channelData       = np.expand_dims(standardizing_the_dataset(x_train_channelData), axis=1)\n",
        "        x_test_channelData        = np.expand_dims(standardizing_the_dataset(x_test_channelData), axis=1)\n",
        "        x_train_preprocessed[:,channel:channel+1, :] = x_train_channelData\n",
        "        x_test_preprocessed[:,channel:channel+1,:]   = x_test_channelData\n",
        "    x_train = x_train_preprocessed\n",
        "    x_test  = x_test_preprocessed \n",
        "     # save orignal y because later we will use binary\n",
        "    y_true  = np.argmax(y_test, axis=1)\n",
        "\n",
        "    #INITIALIZING THE CLASSIFIER TO BE TRAINED ON\n",
        "    input_shape = x_train.shape[1:]\n",
        "    classifier = create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
        "\n",
        "    #FITTING THE CLASSIFIER USING THE PREPROCESSED DATA\n",
        "    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
        "\n",
        "\n",
        "#FOR AFTER FITTING THE MODEL, AND WANT TO LOAD THE MODEL AND ANALYZE ITS PERFORMANCE\n",
        "def fitted_classifier():\n",
        "    #GETTING THE TRAIN/TEST DATASETS\n",
        "    x_train = datasets_dict[dataset_name][0]\n",
        "    y_train = datasets_dict[dataset_name][1]\n",
        "    x_test = datasets_dict[dataset_name][2]\n",
        "    y_test = datasets_dict[dataset_name][3]\n",
        "\n",
        "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
        "\n",
        "    #TRANSFROM THE LABELS FROM INTEGERS TO ONE HOT VECTORS\n",
        "    y_train, y_test = one_hot_encoder(y_train, y_test)\n",
        "\n",
        "    #standardizing the train/test datasets for each channel\n",
        "    x_train_preprocessed = np.zeros((x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
        "    x_test_preprocessed  = np.zeros((x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
        "    channels             = [i for i in range(x_test.shape[1])]\n",
        "    for channel in channels:\n",
        "        x_train_channelData       = x_train[:,channel,:]\n",
        "        x_test_channelData        = x_test[:,channel,:]\n",
        "        x_train_channelData       = np.expand_dims(standardizing_the_dataset(x_train_channelData), axis=1)\n",
        "        x_test_channelData        = np.expand_dims(standardizing_the_dataset(x_test_channelData), axis=1)\n",
        "        x_train_preprocessed[:,channel:channel+1, :] = x_train_channelData\n",
        "        x_test_preprocessed[:,channel:channel+1,:]   = x_test_channelData\n",
        "    x_train = x_train_preprocessed\n",
        "    x_test  = x_test_preprocessed \n",
        "    # save orignal y because later we will use binary\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    #INITIALIZING THE CLASSIFIER TO BE TRAINED ON\n",
        "    input_shape = x_train.shape[1:]\n",
        "    classifier = create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
        "\n",
        "    #LOADING THE MODEL AND EVALUATE IT USING THE TEST DATASET\n",
        "    y_pred, df_metrics = predict(x_test, y_true, x_train, y_train, y_test, output_directory, return_df_metrics=True)\n",
        "\n",
        "    return x_test, y_true, x_train, y_train, y_test, y_pred, df_metrics\n",
        "\n",
        "def predict(x_test, y_true, x_train, y_train, y_test, output_directory, return_df_metrics=True):\n",
        "\n",
        "    #LOADING THE MODEL\n",
        "    start_time = time.time()\n",
        "    model_path = output_directory + 'best_model.h5'\n",
        "    model      = keras.models.load_model(model_path)\n",
        "\n",
        "    #PREDICTING THE SAMPLES FROM THE TEST DATASET\n",
        "    time1  = time.time()\n",
        "    y_pred = model.predict(x_test, batch_size=64)\n",
        "    time2  = time.time()\n",
        "    print(\"time to go through the test set:\", time2-time1)\n",
        "    if return_df_metrics:\n",
        "        y_pred     = np.argmax(y_pred, axis=1)\n",
        "        df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
        "        return y_pred, df_metrics\n",
        "\n",
        "\n",
        "def create_classifier(classifier_name, input_shape, nb_classes, output_directory, verbose=True):\n",
        "  # changed from \"from classifier import fcn\"    to    \"import fcn\"\n",
        "    #GETTING THE CLASSIFIER THAT WE WANT TO BE TRAINED ON THE DATA\n",
        "    if classifier_name == 'resnet':\n",
        "        import resnet\n",
        "        return resnet.Classifier_RESNET(output_directory, input_shape, nb_classes, verbose)\n",
        "    if classifier_name == 'inception':\n",
        "        import inception\n",
        "        return inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose)\n",
        "    if classifier_name == 'eegnet':\n",
        "        import EEGModels\n",
        "        return EEGModels.Classifier_EEGNET(output_directory, input_shape, nb_classes, verbose, nb_epochs=1000)\n",
        "\n",
        "def read_dataset(root_dir, dataset_name):\n",
        "    #LOAD THE DATASET AND SPLIT IT INTO TRAIN/TEST DATASETS\n",
        "    datasets_dict = {}\n",
        "    cur_root_dir  = root_dir\n",
        "    file_name     = cur_root_dir + '/' + dataset_name + '/'\n",
        "    x_train       = np.load(file_name + 'X_train.npy')\n",
        "    y_train       = np.load(file_name + 'y_train.npy')\n",
        "    x_test        = np.load(file_name + 'X_test.npy')\n",
        "    y_test        = np.load(file_name + 'y_test.npy')\n",
        "\n",
        "    datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
        "                                   y_test.copy())\n",
        "    return datasets_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o22ZXnRzmx1u"
      },
      "source": [
        "## Main Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tglLfu2alvx"
      },
      "outputs": [],
      "source": [
        "#--STARTING CODE ---- \n",
        "root_dir = '/content/gdrive/MyDrive/DeepLearningModels_On_EEG_Data/'\n",
        "\n",
        "#DIRECTIONS ON THE DATASET AND CLASSIFIER TO BE USED\n",
        "pathway = []\n",
        "pathway.extend(['BED','eegnet'])  #it can be inception, resnet, eegnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "yu-Xpg9darTC",
        "outputId": "56c31d93-2678-4b38-b8e8-dbd9ccc2e121"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-06bc88c63463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#INITIALIZING VARIABLES FOR CREATING A DIRECTORY TO PUT THE RESULTS OF THE MODEL IN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_name\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mpathway\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclassifier_name\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpathway\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/results/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclassifier_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mdataset_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dir_df_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'df_metrics.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pathway' is not defined"
          ]
        }
      ],
      "source": [
        "#INITIALIZING VARIABLES FOR CREATING A DIRECTORY TO PUT THE RESULTS OF THE MODEL IN\n",
        "dataset_name     = pathway[0]\n",
        "classifier_name  = pathway[1]\n",
        "output_directory = root_dir + '/results/' + classifier_name + '/' +  dataset_name + '/'\n",
        "test_dir_df_metrics = output_directory + 'df_metrics.csv'\n",
        "\n",
        "#IF THE MODEL HAS ALREADY BEEN TRAINED ON, THEN WE CAN LOAD IT AND ANALYSIS ITS PERFORMANCE. IF NOT TRAINED YET, THEN WE CAN TRAIN THE MODEL.\n",
        "if os.path.exists(test_dir_df_metrics):\n",
        "    print('Already done')\n",
        "    loadModelAnswer = input(\"Would you like to load the best model:  (y/n)\")\n",
        "    if loadModelAnswer == 'y':\n",
        "        datasets_dict = read_dataset(root_dir, dataset_name)\n",
        "        x_test, y_true, x_train, y_train, y_test, y_pred, df_metrics = fitted_classifier()\n",
        "        predictions = case_by_case_analysis(y_true, y_pred) #predictions[0] to see the correct and incorrect predictions.\n",
        "        i=1\n",
        "else:\n",
        "    #CREATING THE OUTPUT DIRECTORY WHERE ALL THE RESULTS FROM THE MODEL BEING TRAINED WILL BE LOCATED\n",
        "    create_directory(output_directory)\n",
        "\n",
        "    # READING THE DATASET AND SPLITTING IT INTO TRAIN/TEST DATASETS\n",
        "    datasets_dict = read_dataset(root_dir, dataset_name)\n",
        "\n",
        "    #FITTING THE CLASSIFIER ONTO THE DATASET AND GETTING THE PERFORMANCE OF THE MODEL\n",
        "    fit_classifier()\n",
        "\n",
        "    #CREATING A DIRECTORY TO SAY THAT WE FINISHED TRAINING AND EVALUATING THE MODEL\n",
        "    print('DONE')\n",
        "    create_directory(output_directory + '/DONE')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name     = pathway[0]\n",
        "classifier_name  = pathway[1]\n",
        "output_directory = root_dir + '/results/' + classifier_name + '/' +  dataset_name + '/'\n",
        "test_dir_df_metrics = output_directory + 'df_metrics.csv'\n",
        "datasets_dict = read_dataset(root_dir, dataset_name)"
      ],
      "metadata": {
        "id": "q2xrEg73_3v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa = datasets_dict[dataset_name][2]\n",
        "print(aa.shape)"
      ],
      "metadata": {
        "id": "6hI6_W-sA_7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Deep Learning Networks on EEG data.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}